
\section{Overview}
\texttt{Texture Synthesis using Patch Match}

We present here the recent work of Darabi \cite{Darabi12} - Image Melding - that builds upon Texture Optimization \cite{Kwatra05} with the addition of two new components: the Patch Match algorithm \cite{Barnes09} for fast nearest neighbor search, and the integration of gradient blending to propagate boundary constraints for more efficient image editing \cite{Agarwala04}.

The minimization of Equation \ref{eq:patch_energy} is done by alternating between the two following steps:
\begin{itemize}
	\setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
	\item \textbf{Nearest Neighbor Search}: optimize for $\si$ with $\ti$ fixed, which corresponds to finding the nearest neighbors $\si$ of patches $\ti$ ($\ti$ in result, $\si$ in exemplar)
	\item \textbf{Vote}: given the fixed nearest neighbors $\si$, optimize for the patches $\ti$ in the target result, which corresponds to averaging overlapping patches to get the desired pixels
\end{itemize}

This is done iteratively, using a scale pyramid from coarse to fine scale, until convergence or a given number of steps at each level of the pyramid.
Image Melding also generates gradient information during the voting process and blends it in a third step using the work of \cite{Bhat08}:
\begin{itemize}
	\item \textbf{Screened Poisson Equation}: for the best pixel intensity given current pixel intensities and gradients
\[
	T^*=\argmin_{T} \sum (T - \overline{T})^2 + \lambda \| \nabla T - \overline{\nabla T}\|^2
\]
where $\overline{T}$ and $\overline{\nabla T}$ are the averaged pixel intensities and gradients from the voting step, and $\nabla T$ is the gradient operator applied to the unknown best pixel intensities.
\end{itemize}

%-------------------------------------------------------------------------
\texttt{Project Proposal}

Given the plethora of applications using texture synthesis and the many variations we presented, we propose a new target for exemplar-based texture synthesis: synthesizing stereo-view video from a single-view video stream (for example to view it in 3D).

\subsection{Traditional stereo methods}

The traditional multi-view approach consist in partially modeling the 3D environment such as with Hyper-lapse videos \cite{Kopf14} or stereo reconstruction \cite{Seitz06}. A naive approach for stereo synthesis would therefore be to use video frame correspondences to register the camera path and content geometry, which would provide a way to synthesize stereo-view video.

\subsection{Our approach}

Since the many steps required by multi-view reconstruction do not always work well and accumulate errors, we propose a much simpler approach based on texture synthesis to generate the second stream of a stereo-view video stream using patch correspondences.

The basic idea consists of having a set of true stereo videos (seemingly available \cite{Corrigan10, Smolic10}) as exemplar frames.
Given the pair of true streams $L$ and $R$ (left and right image streams) and a new single-view stream $L'$, we can synthesize its corresponding part $R'$ by
\begin{enumerate}
	\setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
	\item Finding the nearest neighbor patches of $L'$ in $L$
	\item Using the corresponding patches in $R$ for $R'$
\end{enumerate}
which corresponds to image analogies \cite{Efros01, Hertzmann01} applied to stereo video streams.

The potential new problems to deal with include
\begin{itemize}
	\setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
	\item Dealing with time coherence efficiently (either using 3D spatio-temporal patches or per-frame 2D patches with some extra coherence constraint)
	\item Retrieving patches from multiple exemplars \cite{Barnes11, Hu13} (i.e. multiple frames and a collection of video streams)
\end{itemize}

Framework:
\begin{itemize}
	\item NNF from A' to A
	\item Stereo transfer (style, difference, disparity, etc.)
	\item Multi-scale pyramid / Laplacian pyramid
\end{itemize}